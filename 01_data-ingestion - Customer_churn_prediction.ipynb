{"cells":[{"cell_type":"markdown","source":["# Create, evaluate, and score a churn prediction model"],"metadata":{},"id":"98e321e3-7797-4a8d-9822-2f1084ba2e2f"},{"cell_type":"markdown","source":["## Introduction\n\nIn this notebook series, you'll see a Microsoft Fabric data science workflow with an end-to-end example. The scenario is to build a model to predict whether bank customers would churn or not. The churn rate, also known as the rate of attrition refers to the rate at which bank customers stop doing business with the bank.\n\nThe main steps in this notebook series are:\n\n**Notebook 1: Data Ingestion** <br>\n&nbsp; &nbsp; 1. Install custom libraries <br>\n&nbsp; &nbsp; 2. Load the data <br>\n\n**Notebook 2: Data Preparation** <br>\n&nbsp; &nbsp; 3. Understand and process the data through exploratory data analysis and demonstrate the use of Fabric Data Wrangler feature.\n\n**Notebook 3: Model Training**<br>\n&nbsp; &nbsp; 4. Train machine learning models using `Scikit-Learn` and `LightGBM`, and track experiments using MLflow and Fabric Autologging feature.<br>\n&nbsp; &nbsp; 5. Evaluate and save the final machine learning model.<br>\n\n**Notebook 4: Inference**<br>\n&nbsp; &nbsp; 6. load the best model to run predicitons.<br>\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"faa52bd2-a2ca-4cc4-90c2-e37b69df136d"},{"cell_type":"markdown","source":["## Prerequisites\n","- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will be downloading data from a public blob, then storing the data in the lakehouse. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1eb8ee2b-ba0c-43b3-9d9d-8654067ec748"},{"cell_type":"markdown","source":["## Step 1: Install custom libraries\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d198b4dc-fd92-441f-9df2-5fe3266d09f3"},{"cell_type":"markdown","source":["When developing a machine learning model or doing ad-hoc data analysis, you may need to quickly install a custom library (e.g., `imblearn` in this notebook) for the Apache Spark session. To do this, you have two choices. \n\n1. You can use the in-line installation capabilities (e.g., `%pip`, `%conda`, etc.) to quickly get started with new libraries. Note that this installation option would install the custom libraries only in the current notebook and not in the workspace.\n\n```python\n# Use pip to install libraries\n%pip install <library name>\n\n# Use conda to install libraries\n%conda install <library name>\n \n```\n2. Alternatively, you can create a Fabric environment, install libraries from public sources or upload custom libraries to it, and then your workspace admin can attach the environment as the default for the workspace. All the libraries in the environment will then become available for use in any notebooks and Spark job definitions in the workspace. For more information on environments, see [create, configure, and use an environment in Microsoft Fabric](https://aka.ms/fabric/create-environment)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"24e40fc3-322e-4f19-9218-1636d3f9e082"},{"cell_type":"markdown","source":["For this notebook, you'll install the `imblearn` using `%pip install`. Note that the PySpark kernel will be restarted after `%pip install`, thus you'll need to install the library before you run any other cells."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"591a8c19-19ab-49ce-b1b8-7c3cbfe9a00a"},{"cell_type":"code","source":["# Use pip to install imblearn for SMOTE\n","# %pip install imblearn --quiet"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8b5da681-e3ed-4ddc-8ff1-5cea41a3f98a"},{"cell_type":"markdown","source":["## Step 2: Ingest data into a Microsoft Fabric lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7eccc566-de46-4769-91d1-0c68256c90a4"},{"cell_type":"markdown","source":["### Bank churn dataset\n\nThe dataset in *churn.csv* contains the churn status of 10,000 customers, along with 14 attributes that include:\n\n- Credit score\n- Geographical location (Germany, France, Spain)\n- Gender (male, female)\n- Age\n Tenure (number of years the person was a customer at that bank)\n- Account balance\n- Estimated salary\n- Number of products that a customer purchased through the bank\n- Credit card status (whether or not the customer has a credit card)\n- Active member status (whether or not the person is an active bank customer)\n\nThe dataset also includes row number, customer ID, and customer surname columns. Values in these columns shouldn't influence a customer's decision to leave the bank.\n\nA customer bank account closure event defines the churn for that customer. The dataset `Exited` column refers to the customer's abandonment. Since we have little context about these attributes, we don't need background information about the dataset. We want to understand how these attributes contribute to the `Exited` status.\n\nOut of those 10,000 customers, only 2037 customers (roughly 20%) left the bank. Because of the class imbalance ratio, we recommend generation of synthetic data. Confusion matrix accuracy might not have relevance for imbalanced classification. We might want to measure the accuracy using the Area Under the Precision-Recall Curve (AUPRC).\n\n- This table shows a preview of the `churn.csv` data:\n\n|CustomerID|Surname|CreditScore|Geography|Gender|Age|Tenure|Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|15634602|Hargrave|619|France|Female|42|2|0.00|1|1|1|101348.88|1|\n|15647311|Hill|608|Spain|Female|41|1|83807.86|1|0|1|112542.58|0|"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"90eec3b3-1a5d-4a3a-bfbd-78b0b035f7a9"},{"cell_type":"markdown","source":["### Imports and parameters"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c130fce6-b255-461e-ad2e-9591031f6b83"},{"cell_type":"code","source":["import os\n","import requests\n","from pathlib import Path, PurePath, PurePosixPath"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ccb97e27-60bc-4013-bfcf-dfcb3f918558"},{"cell_type":"markdown","source":["Define these parameters, so that you can use this notebook with different datasets or [Assign parameters values from a pipeline](https://learn.microsoft.com/en-us/fabric/data-engineering/author-execute-notebook#assign-parameters-values-from-a-pipeline)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"065a83b5-6b29-402f-9359-4ae3d9a74ebc"},{"cell_type":"code","source":["# Specify the storage location for the data set\n","DATA_ROOT = \"/lakehouse/default\"\n","DATA_FOLDER = \"Files/churn\"  # Folder with data files\n","DATA_FILE = \"churn.csv\"  # Data file name\n","REMOTE_URL = \"https://synapseaisolutionsa.blob.core.windows.net/public/bankcustomerchurn\" #URL of demo data\n","OUTPUT_TABLE_NAME = \"churn\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"bcd5f6a9-2008-4a06-83cd-a0874797a801"},{"cell_type":"markdown","source":["### Download dataset and upload to lakehouse"],"metadata":{},"id":"26e0b468"},{"cell_type":"code","source":["# With an Azure Synapse Analytics blob, this can be done in one line\n","\n","# Download demo data files into lakehouse if not exist\n","file_list = [DATA_FILE]\n","download_path = PurePosixPath(DATA_ROOT, DATA_FOLDER, 'raw')\n","\n","if not os.path.exists(\"/lakehouse/default\"):\n","    raise FileNotFoundError(\n","        \"Default lakehouse not found, please add a lakehouse and restart the session.\"\n","    )\n","os.makedirs(download_path, exist_ok=True)\n","for fname in file_list:\n","    if not os.path.exists(Path(download_path, fname)):\n","        r = requests.get(f\"{REMOTE_URL}/{fname}\", timeout=30)\n","        with open(Path(download_path, fname), \"wb\") as f:\n","            f.write(r.content)\n","        print(\"Downloaded demo data files into lakehouse.\")\n","    else: \n","        print(\"Demo data files already exists.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a89670b9-66cb-4679-84a1-bf36923d26a6"},{"cell_type":"markdown","source":["### Read raw data from the lakehouse\n","\n","This code reads raw data from the **Files** section of the lakehouse, and adds more columns for different date parts. Creation of the partitioned delta table uses this information."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21d1dea8-b772-43d5-b36a-c2369b84302c"},{"cell_type":"code","source":["df = (\n","    spark.read.option(\"header\", True)\n","    .option(\"inferSchema\", True)\n","    .csv(str(PurePath(DATA_FOLDER, 'raw', DATA_FILE)))\n","    .cache()\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2ad87cb-a5f9-4a5b-a13d-661210d821c7"},{"cell_type":"markdown","source":["### Create a delta table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e94b900d-b8ca-4902-8fb5-3fb596514316"},{"cell_type":"code","source":["# Create a PySpark DataFrame from pandas\n","output_table_name = f\"bronze/{OUTPUT_TABLE_NAME}\"\n","df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{output_table_name}\")\n","print(f\"Spark DataFrame saved to delta table: {output_table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d46f686f-10f7-4a32-8122-616e3d58299a"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aa65aeef-c5b7-458b-89bb-52fbd879b837"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"},"enableDebugMode":false}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}