{"cells":[{"cell_type":"markdown","source":["# Create, evaluate, and score a churn prediction model"],"metadata":{},"id":"98e321e3-7797-4a8d-9822-2f1084ba2e2f"},{"cell_type":"markdown","source":["## Introduction\n\nIn this notebook series, you'll see a Microsoft Fabric data science workflow with an end-to-end example. The scenario is to build a model to predict whether bank customers would churn or not. The churn rate, also known as the rate of attrition refers to the rate at which bank customers stop doing business with the bank.\n\nThe main steps in this notebook series are:\n\n**Notebook 1: Data Ingestion** <br>\n&nbsp; &nbsp; 1. Install custom libraries <br>\n&nbsp; &nbsp; 2. Load the data <br>\n\n**Notebook 2: Data Preparation** <br>\n&nbsp; &nbsp; 3. Understand and process the data through exploratory data analysis and demonstrate the use of Fabric Data Wrangler feature.\n\n**Notebook 3: Model Training**<br>\n&nbsp; &nbsp; 4. Train machine learning models using `Scikit-Learn` and `LightGBM`, and track experiments using MLflow and Fabric Autologging feature.<br>\n&nbsp; &nbsp; 5. Evaluate and save the final machine learning model.<br>\n\n**Notebook 4: Inference**<br>\n&nbsp; &nbsp; 6. load the best model to run predicitons.<br>\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"faa52bd2-a2ca-4cc4-90c2-e37b69df136d"},{"cell_type":"markdown","source":["## Prerequisites\n","- [Add a lakehouse](https://aka.ms/fabric/addlakehouse) to this notebook. You will be downloading data from a public blob, then storing the data in the lakehouse. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1eb8ee2b-ba0c-43b3-9d9d-8654067ec748"},{"cell_type":"markdown","source":["### Imports and Parameters"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e312c99d-d5f7-4cc8-93ad-17bb72df444e"},{"cell_type":"code","source":["import joblib\n","import inspect\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import mlflow"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f1b2017d-1005-49ec-985d-f0921994515c"},{"cell_type":"markdown","source":["Define these parameters, so that you can use this notebook with different datasets or [Assign parameters values from a pipeline](https://learn.microsoft.com/en-us/fabric/data-engineering/author-execute-notebook#assign-parameters-values-from-a-pipeline)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"372fb863-0f73-4a35-bb11-dc22256964bc"},{"cell_type":"code","source":["TARGET = \"Exited\" # Dependent (target) attribute\n","EXPERIMENT_NAME = \"bank-churn\"  # Mlflow  experiment name\n","INPUT_TABLE_NAME = \"silver/churn\"\n","OUTPUT_TABLE_PREFIX = \"churn\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"8024548c-a089-4d50-b2b3-2fa989ec7568"},{"cell_type":"markdown","source":["## Step 4: Model training and tracking\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"80113f25-b8d2-4d79-ac84-6c396dedb539"},{"cell_type":"markdown","source":["With the data in place, you can now define the model. Apply random forest and LightGBM models in this notebook.\n","\n","Use the scikit-learn and LightGBM libraries to implement the models, with a few lines of code. Additionally, use MLfLow and Fabric Autologging to track the experiments.\n","\n","This code sample loads the delta table from the lakehouse. You can use other delta tables that themselves use the lakehouse as the source."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a5d1fffa-b0fb-4ac8-9d7f-41339ca50faf"},{"cell_type":"code","source":["df_clean = spark.read.format(\"delta\").load(f\"Tables/{INPUT_TABLE_NAME}\").toPandas()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"025e525b-b00c-4537-9edb-cc445782d736"},{"cell_type":"markdown","source":["### Generate an experiment for tracking and logging the models by using MLflow\n","\n","This section shows how to generate an experiment, and it specifies the model and training parameters and the scoring metrics. Additionally, it shows how to train the models, log them, and save the trained models for later use."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"eef9eead-c909-4cb2-848f-12d2ad6a3df3"},{"cell_type":"markdown","source":["### Set experiment and autologging specifications\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"65b8693b-2970-4daf-ad31-067a25b80b60"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"65ef9b35-9f44-4947-b239-294b2f0441f9"},{"cell_type":"code","source":["mlflow.set_experiment(EXPERIMENT_NAME)\n","mlflow.autolog(exclusive=False)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b08b40bc-e4c1-4faf-a388-2ed301580d16"},{"cell_type":"markdown","source":["Autologging automatically captures both the input parameter values and the output metrics of a machine learning model, as that model is trained. This information is then logged to your workspace, where the MLflow APIs or the corresponding experiment in your workspace can access and visualize it.\n","\n","When complete, your experiment resembles this image:\n","\n","<img src=\"https://sdkstorerta.blob.core.windows.net/churnblob/experiment_runs.png\"  width=\"70%\" height=\"10%\" title=\"Screenshot shows the experiment page for the bank-churn-experiment.\">\n","\n","All the experiments with their respective names are logged, and you can track their parameters and performance metrics. To learn more about autologging, see [Autologging in Microsoft Fabric](https://aka.ms/fabric-autologging).\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6b415622-97f7-480f-bc72-9f63053742fc"},{"cell_type":"markdown","source":["### Import scikit-learn and LightGBM"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"634163fa-5062-469e-a5e9-ed49ef7e4607"},{"cell_type":"code","source":["\n","# Import the required libraries for data transformation\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import make_column_transformer\n","from sklearn.preprocessing import KBinsDiscretizer, StandardScaler, MinMaxScaler\n","\n","# Import the required libraries for model training\n","from sklearn.model_selection import train_test_split\n","from lightgbm import LGBMClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, recall_score, roc_auc_score, classification_report"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ffbdd7fd-a273-458c-9cea-dbcd583911dc"},{"cell_type":"markdown","source":["### Prepare training, validation and test datasets"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c72af93c-971d-4b13-9079-0025a5d78b32"},{"cell_type":"code","source":["y = df_clean[TARGET]\n","X = df_clean.drop(TARGET, axis=1)\n","# Train/test separation\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1, stratify=y)\n","# Train-Validation Separation\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1, stratify=y_train)\n","\n","print(f\"Training dataset: {len(X_train)} | {100* len(X_train)/len(df_clean)}%\")\n","print(f\"Validation dataset: {len(X_val)} | {100* len(X_test)/len(df_clean)}%\")\n","print(f\"Test dataset: {len(X_test)} | {100* len(X_test)/len(df_clean)}%\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3c539eed-0f88-4f0a-b7d2-901c2f9930e3"},{"cell_type":"markdown","source":["### Save test data\n","\n","Save the test data to delta table for model evaluation and use in next notebook."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a187c0d-681a-46cd-a883-1aee333bd0e0"},{"cell_type":"code","source":["table_name = f\"gold/{OUTPUT_TABLE_PREFIX}_test\"\n","# Create PySpark DataFrame from Pandas\n","df_test=spark.createDataFrame(X_test)\n","df_test.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(f\"Tables/{table_name}\")\n","print(f\"Spark test DataFrame saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e3e48468-0cb0-48c0-91f3-cf64cb330663"},{"cell_type":"markdown","source":["### Apply SMOTE to the training data\n","\n","Imbalanced classification has a problem, because it has too few examples of the minority class for a model to effectively learn the decision boundary. To handle this, Synthetic Minority Oversampling Technique (SMOTE) is the most widely used technique to synthesize new samples for the minority class. Access SMOTE with the `imblearn` library that you installed in step 1.\n","\n","<mark>Apply SMOTE only to the training dataset</mark>. You must leave the test dataset in its original imbalanced distribution, to get a valid approximation of model performance on the original data. This experiment represents the situation in production.\n","\n","For more information, see [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html#) and [From random over-sampling to SMOTE and ADASYN](https://imbalanced-learn.org/stable/over_sampling.html#smote-adasyn). The imbalanced-learn website hosts these resources."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c907a17c-ed92-4334-accc-2deec809334c"},{"cell_type":"code","source":["from collections import Counter\n","from imblearn.over_sampling import SMOTE\n","\n","sm = SMOTE(random_state=1)\n","X_res, y_res = sm.fit_resample(X_train, y_train)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bdbb6dec-3e17-498a-8e8d-380bc9a59f5a"},{"cell_type":"markdown","source":["### Train the models\n","\n","Use Random Forest to train the model, with a maximum depth of four, and with four features:"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"039ad12c-e176-4162-80da-f1cd15becaf2"},{"cell_type":"code","source":["model_name = \"rfc1_sm\"\n","# Register the trained model with autologging\n","mlflow.sklearn.autolog(\n","    registered_model_name=model_name, \n","    silent=True, \n","    log_input_examples=True,\n","    log_models=True)\n","\n","# Create the model\n","rfc1_model = RandomForestClassifier(\n","    max_depth=4,\n","    max_features=4,\n","    min_samples_split=3,\n","    random_state=1,\n",")    \n","with mlflow.start_run(run_name=model_name) as run:\n","    # Capture run_id for model prediction later\n","    rfc1_run_id = run.info.run_id\n","    print(f\"run_id: {rfc1_run_id}; status: {run.info.status}\")\n","    \n","    # fit with balanced training data\n","    rfc1_model.fit(X_res, y_res.ravel())\n","    rfc1_model.score(X_val, y_val)\n","    y_pred = rfc1_model.predict(X_val)\n","    cr = classification_report(y_val, y_pred)\n","    cm = confusion_matrix(y_val, y_pred)\n","    roc_auc = roc_auc_score(y_val, rfc1_model.predict_proba(X_val)[:, 1])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"58be6139-1499-43d2-8ed1-3ca0bcda6498"},{"cell_type":"markdown","source":["### View the experiment artifact to track model performance\n","\n","The experiment runs are automatically saved in the experiment artifact. You can find that artifact in the workspace. An artifact name is based on the name used to set the experiment. All of the trained models, their runs, performance metrics and model parameters are logged on the experiment page.\n","\n","To view your experiments:\n","1. On the left panel, select your workspace.\n","1. Find and select the experiment name, in this case, **bank-churn-experiment**.\n","\n","<img src=\"https://sdkstorerta.blob.core.windows.net/churnblob/experiment_runs.png\"  width=\"80%\" title=\"Screenshot shows logged values for one of the models.\">"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"449f8d4d-04ac-43a8-a6e4-1c10ace0c376"},{"cell_type":"markdown","source":["## Specific data preprocessing\n","\n","Some models or data might require specific preprocessing step which depend on the distribution of the training data. Because data distribution might change over time (data drift), it is recommended to manage those transformation steps with the model, not sharing them with other experiments. We use `sklearn.compose.ColumnTransformer` to create the feature transformations, such as scaling or binning. Later we include those transformations with the model in an `sklean.pipeline`. "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0bf22d6b-1747-4677-aa13-aea65674c412"},{"cell_type":"code","source":["def get_transformers():\n","    col_transformer = make_column_transformer(\n","            (MinMaxScaler(feature_range=(0, 21)), [\"Tenure\", \"CreditScore\"]),\n","            (StandardScaler(), [\"CreditScore\", \"Age\"]),\n","            (KBinsDiscretizer(n_bins=21, encode=\"ordinal\", strategy=\"quantile\"), [\"EstimatedSalary\", \"Balance\"]),\n","        remainder=\"passthrough\"\n","    )\n","    return col_transformer\n","\n","get_transformers()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f1b76ad5-64ea-43a1-9d8d-d5626a485326"},{"cell_type":"markdown","source":["### Train a model pipeline\n","Specify a data training objective function "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a5e6314a-4610-47fd-88de-3800f023c1c2"},{"cell_type":"code","source":["from mlflow.models import infer_signature\n","\n","\n","def train_pipeline(model, run_name: str, **fit_kwargs):\n","    with mlflow.start_run(run_name=run_name) as run:\n","        # Capture run_id for model prediction later\n","        run_id = run.info.run_id\n","        print(f\"run_id: {run_id}; status: {run.info.status}\")\n","        \n","        # Fit and run data transformation  \n","        transfomers = get_transformers()\n","        transfomers.fit(X_train)\n","        X_res_tf = transfomers.transform(X_res)\n","        X_val_tf = transfomers.transform(X_val)\n","        \n","        # Add feature names to fit_kwargs, if supported by method\n","        sig = inspect.signature(model.fit)\n","        if 'feature_name' in sig.parameters:\n","            fit_kwargs['feature_name'] = transfomers.get_feature_names_out().tolist()\n","\n","        # fit with balanced training data\n","        model.fit(X_res_tf, y_res.ravel(), **fit_kwargs)\n","        model.score(X_val_tf, y_val)\n","        y_pred = model.predict(X_val_tf)\n","        cr = classification_report(y_val, y_pred)\n","        cm = confusion_matrix(y_val, y_pred)\n","        roc_auc = roc_auc_score(y_val, model.predict_proba(X_val_tf)[:, 1])\n","\n","        # log the sklearn model pipeline artifact to mlflow\n","        signature = infer_signature(model_input=X_train, model_output=y_pred)\n","        mlflow.sklearn.log_model(\n","            sk_model=Pipeline([\n","                ('preprocessor', transfomers),\n","                ('classifier', model)]),\n","            artifact_path='model',\n","            signature=signature,\n","            registered_model_name=run_name)\n","        return run_id"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0f77e084-679b-4dc5-9a81-6ad480e08f39"},{"cell_type":"markdown","source":["Use Random Forest to train the model, with a maximum depth of eight, and with six features:"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"562edcf7-8926-4757-ad5a-871a1f813772"},{"cell_type":"code","source":["model_name = \"rfc2_sm\"\n","# Register the trained model with autologging\n","mlflow.sklearn.autolog(\n","    silent=True, \n","    log_input_examples=False,\n","    log_models=False)\n","\n","# Create the model\n","rfc2_model = RandomForestClassifier(\n","    max_depth=8, \n","    max_features=6, \n","    min_samples_split=3, \n","    random_state=1,\n",")\n","\n","rfc2_run_id = train_pipeline(model=rfc2_model, run_name=model_name);"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"18ab69b4-7098-4bfb-a393-fcd62a2bc0b4"},{"cell_type":"markdown","source":["Train the model with LightGBM.\n","\n","><mark>NOTE:</mark>\\\n",">We change the mlflow flavor for autolog because LightGBM has its own model flavor.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"46086429-c184-46b8-872b-58632eae583e"},{"cell_type":"code","source":["# lgbm_model\n","model_name = \"lgbm_sm\"\n","# Register the trained model with autologging\n","mlflow.lightgbm.autolog(\n","    silent=True, \n","    log_input_examples=False,\n","    log_models=False)\n","# Create the model  \n","lgbm_model = LGBMClassifier(\n","    learning_rate=0.07,\n","    max_delta_step=2,\n","    n_estimators=100,\n","    max_depth=10,\n","    eval_metric=\"logloss\",\n","    objective=\"binary\",\n","    random_state=42,\n","    verbose=-1,\n",")\n","# combine transformation and model to an estimator pipeline \n","\n","lgbm_run_id = train_pipeline(model=lgbm_model, run_name=model_name);"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f7d92a2-17ea-43b0-b17c-b3fac5b76af6"},{"cell_type":"markdown","source":["## Step 5: Evaluate the final machine learning models\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"90431a0d-205d-4352-9205-c98ef3b905c4"},{"cell_type":"markdown","source":["With Mlflow, models can be shared and versioned without having to reference the experiment or run. For this purpose, the name and version of the model are stored and managed in the model registry. The model registry enables secure access to models beyond your own workspace.\n\nBy specifying the `registered_model_name` attribute, we have already registered our models in the model registry during training. This enables us to access the models created here in the subsequent inference step.\n\nTo view your registered models:\n\n1. On the left panel, select your workspace.\n2. Find and select the model name, in this case, **lgbm_sm**.\n\n\n><mark>NOTE: </mark> \\\n>If your models are registered for the first time, you might have to refresh your browser window in order to see them.\\\n>If you only see the experiments but not the model, it means that the models are not registered. You can \n> - use the `mlflow.model_register()` function to register your models (see: [mlflow API documentation](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.register_model)). \n> - manually got to the experiment from the workspace, select the best model and click **Save as ML model**. This will register the model in your workspace under a name you provided.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6f0aff92-bd7f-4f10-af1b-6617c1ee3f0c"},{"cell_type":"markdown","source":["### Load models by model URI"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1f7db87d-df62-4a4f-91ca-4d0eca49d4cc"},{"cell_type":"markdown","source":["All models from the experiment are accessible in your workspace by calling them by the corresponding model URI:\n","\n","><mark>NOTE: </mark> \\\n",">The `mlflow.sklearn` model flavor is used to load the models because they are wrapped in `sklearn.pipeline` objects. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c61dd49a-dbc4-41b2-9065-58acd9072a5a"},{"cell_type":"code","source":["# Fetch the model from the run_uri\n","load_model_rfc1 = mlflow.sklearn.load_model(f\"runs:/{rfc1_run_id}/model\")\n","load_model_rfc2 = mlflow.sklearn.load_model(f\"runs:/{rfc2_run_id}/model\")\n","load_model_lgbm = mlflow.sklearn.load_model(f\"runs:/{lgbm_run_id}/model\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6f5dd987-2bc0-4ece-839d-d1a1fe66b6f1"},{"cell_type":"markdown","source":["### Assess the performance of the saved models on the testing dataset"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5803d3b4-3c9f-4d62-bc2a-79f841ec45cc"},{"cell_type":"code","source":["# Random forest with maximum depth of 4 and 4 features\n","ypred_rfc1 = load_model_rfc1.predict(X_val)\n","# Random forest with maximum depth of 8 and 6 features\n","ypred_rfc2 = load_model_rfc2.predict(X_val)\n","# LightGBM\n","ypred_lgbm = load_model_lgbm.predict(X_val)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0181064b-34b6-4f26-879d-9d145edfac06"},{"cell_type":"markdown","source":["### Show true/false positives/negatives by using a confusion matrix\n","\n","To evaluate the accuracy of the classification, build a script that plots the confusion matrix. You can also plot a confusion matrix using SynapseML tools, as shown in the [Fraud Detection sample](https://aka.ms/samples/frauddectection)."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"97eb022f-cf7a-4b1e-8c5c-d0c5dfe4a9f1"},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","def plot_confusion_matrix(estimator, X, y, title=\"Confusion matrix\", **kwargs):\n","    fig, ax = plt.subplots(figsize=(4, 4))\n","    disp = ConfusionMatrixDisplay.from_estimator(estimator, X, y,\n","            display_labels=['Non Churn','Churn'] ,\n","            labels=sorted(y.unique()), xticks_rotation=45, cmap='Blues', ax=ax)\n","    plt.yticks(rotation=45)\n","    plt.title(title, fontsize=10)\n","    plt.show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"75b9e711-ddb4-47e8-9796-1724707293b1"},{"cell_type":"code","source":["# Computing and display the confusion matrix\n","plot_confusion_matrix(\n","    estimator=load_model_rfc1,\n","    X=X_val,\n","    y=y_val,\n","    title=\"Random Forest with max depth of 4\",\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9933123f-38d8-4fca-a8ab-ec09436a1754"},{"cell_type":"markdown","source":["Create a confusion matrix for the random forest classifier with maximum depth of eight, with six features:"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"50a20d7f-d9cb-42ca-a537-1b50894bedfb"},{"cell_type":"code","source":["# Computing and display the confusion matrix\n","plot_confusion_matrix(\n","    estimator=load_model_rfc2,\n","    X=X_test,\n","    y=y_test,\n","    title=\"Random Forest with max depth of 8\",\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c5fefc40-de64-4480-8f21-0efd11538f93"},{"cell_type":"markdown","source":["Create a confusion matrix for LightGBM:"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ef73db99-3670-44cb-8e15-f3885ca9a02b"},{"cell_type":"code","source":["plot_confusion_matrix(\n","    estimator=load_model_lgbm,\n","    X=X_test,\n","    y=y_test,\n","    title=\"LightGBM\",\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05052942-cb51-461e-861c-8312bc6182d8"},{"cell_type":"markdown","source":["-----\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4f713a13-3874-4fbb-9ed6-870cbce5f814"},{"cell_type":"markdown","source":["### Save results for Power BI\n","\n","Save the delta frame to the lakehouse, to move the model prediction results to a Power BI visualization."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a23cb929-aa96-49f4-8d16-85b93fab6a51"},{"cell_type":"code","source":["df_pred = X_test.copy()\n","df_pred['y_test'] = y_test\n","df_pred['ypred_rfc1'] = ypred_rfc1\n","df_pred['ypred_rfc2'] =ypred_rfc2\n","df_pred['ypred_lgbm'] = ypred_lgbm\n","table_name = \"df_pred_results\"\n","sparkDF=spark.createDataFrame(df_pred)\n","sparkDF.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").save(f\"Tables/gold/{table_name}\")\n","print(f\"Spark DataFrame saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2b27a0b3-5647-463f-b26e-4edbf44bcde8"},{"cell_type":"markdown","source":["## Optional: Access visualizations in Power BI"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0d503f0e-6758-47ea-b836-8bc955663b20"},{"cell_type":"markdown","source":["Access your saved table in Power BI:\n\n1. On the left, select **OneLake data hub**\n1. Select the lakehouse that you added to this notebook\n1. In the **Open this Lakehouse** section, select **Open**\n1. On the ribbon, select **New semantic model**. Select `df_pred_results`, and then select **Continue** to create a new Power BI semantic model linked to the predictions\n1. Select **New report** from the tools at the top of the semantic models page, to open the Power BI report authoring page\n\nThe following screenshot shows some example visualizations. The data panel shows the delta tables and columns to select from a table. After selection of appropriate category (x) and value (y) axis, you can choose the filters and functions - for example, sum or average of the table column.\n\n> [!NOTE]\n> In this screenshot, the illustrated example describes the analysis of the saved prediction results in Power BI:\n\n<img src=\"https://synapseaisolutionsa.blob.core.windows.net/public/bankcustomerchurn/PBIviz3.png\"  width=\"100%\" height=\"100%\" title=\"Screenshot shows a Power BI dashboard example.\">"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5a9e488f-cd7f-4076-a90b-2e8687aa3035"},{"cell_type":"markdown","source":["However, for a real customer churn use-case, the user might need a more thorough set of requirements of the visualizations to create, based on subject matter expertise, and what the firm and business analytics team and firm have standardized as metrics.\n\nThe Power BI report shows that customers who use more than two of the bank products have a higher churn rate. However, few customers had more than two products. (See the plot in the bottom left panel.) The bank should collect more data, but should also investigate other features that correlate with more products.\n\nBank customers in Germany have a higher churn rate compared to customers in France and Spain. (See the plot in the bottom right panel). Based on the report results, an investigation into the factors that encouraged customers to leave might help.\n\nThere are more middle aged customers (between 25-45) and customers between 45-60 tend to exit more.\n\nFinally, customers with lower credit scores would most likely leave the bank for other financial institutions. The bank should explore ways to encourage customers with lower credit scores and account balances to stay with the bank."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"61e3d32f-7fe4-4c31-ba4f-593ec631c542"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"synapse_widget":{"version":"0.1","state":{}},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"},"enableDebugMode":false}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}